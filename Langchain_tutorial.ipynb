{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6f94c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a079240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc4f1c8",
   "metadata": {},
   "source": [
    "### INSTANCIANDO EL MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06faedd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model = \"gpt-4o\",\n",
    "                 temperature = 0.7, # mas cerca de 0 mas concreto mas cerca de 1 mas creativo\n",
    "                 verbose = True                \n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c95c00",
   "metadata": {},
   "source": [
    "### INVOKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8dce642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬°Hola! Estoy bien, gracias. ¬øY t√∫? ¬øEn qu√© puedo ayudarte hoy?\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"Hola, como estas?\") # invoke solo acepta una pregunta\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e59bb7f",
   "metadata": {},
   "source": [
    "### BATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "077dcef6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬°Hola! Estoy aqu√≠ para ayudarte. ¬øEn qu√© puedo asistirte hoy?\n",
      "**************************\n",
      "En el coraz√≥n de Sudam√©rica, un tesoro escondido,\n",
      "Bolivia, tierra de alturas y misterios compartidos,\n",
      "Donde el cielo se encuentra con la tierra en un abrazo sin fin,\n",
      "Y el alma de su gente resuena en cada rinc√≥n y conf√≠n.\n",
      "\n",
      "Desde el altiplano, donde el aire es cristalino y puro,\n",
      "Hasta las selvas rebosantes, donde los r√≠os murmuran su canto maduro,\n",
      "El Salar de Uyuni, espejo del cielo, vasto y brillante,\n",
      "Refleja sue√±os y esperanzas en un paisaje deslumbrante.\n",
      "\n",
      "La Paz, ciudad de alturas, donde las monta√±as son guardianas,\n",
      "Mirando con ojos antiguos, historias de luchas y haza√±as,\n",
      "El Illimani vigila, con su manto nevado y eterno,\n",
      "Mientras las calles vibran con vida, en un ritmo diurno y nocturno.\n",
      "\n",
      "El lago Titicaca, sagrado y sereno, cuna de leyendas,\n",
      "Sus aguas profundas guardan secretos y ofrendas,\n",
      "Islas del Sol y de la Luna, con historias de dioses y reyes,\n",
      "Donde el tiempo parece detenerse, en sus m√°gicos atardeceres.\n",
      "\n",
      "Cochabamba, Santa Cruz, Potos√≠ y Sucre, nombres que resuenan,\n",
      "Cada regi√≥n con su esencia, cada pueblo con su pena,\n",
      "La mina de plata que aliment√≥ imperios, la ciudad blanca y su historia,\n",
      "Bolivia, mosaico de culturas, en una danza de gloria.\n",
      "\n",
      "Tus mercados llenos de colores, tus tejidos y artesan√≠as,\n",
      "Son el reflejo de un pueblo que en sus manos guarda vidas,\n",
      "La m√∫sica de los Andes, con charangos y zampo√±as,\n",
      "Canta al viento y al destino, en melod√≠as que nos asombran.\n",
      "\n",
      "Bolivia, tierra de contrastes, de lo antiguo y lo nuevo,\n",
      "En tus valles y monta√±as, se entrelazan los sue√±os,\n",
      "Eres un canto a la resistencia, un poema de esperanza,\n",
      "Un lugar donde el pasado y el futuro se abrazan.\n",
      "\n",
      "Que tu esp√≠ritu indomable siga brillando con vigor,\n",
      "En cada rinc√≥n de tu geograf√≠a, en cada gesto de amor,\n",
      "Bolivia, joya del altiplano, joya de Sudam√©rica,\n",
      "En tus paisajes y tu gente, encontramos nuestra Am√©rica.\n"
     ]
    }
   ],
   "source": [
    "response = llm.batch([\"Hola, como estas?\", \"Escribe un poema sobre de Bolivia\"])\n",
    "print(response[0].content)  # batch acepta varias preguntas a la vez y las corre en paralelo\n",
    "print(\"**************************\")\n",
    "print(response[1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647fecfe",
   "metadata": {},
   "source": [
    "### STREAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d685d4d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En el coraz√≥n de Sudam√©rica, all√≠ se encuentra,\n",
      "Una tierra de monta√±as, selvas y paz eterna,\n",
      "Bolivia, joya andina, con tu alma tan profunda,\n",
      "De la Pachamama eres hija, con historia fecunda.\n",
      "\n",
      "Desde el altiplano donde el c√≥ndor vuela alto,\n",
      "Hasta los valles verdes donde el r√≠o canta bajo,\n",
      "Tu diversidad es vasta, tu cultura es un tesoro,\n",
      "Un crisol de tradiciones que en tus venas atesoro.\n",
      "\n",
      "En el tit√°nico Titicaca, espejo del cielo azul,\n",
      "Navegan leyendas antiguas, de un pueblo que es fiel,\n",
      "Aymaras y quechuas, con sabidur√≠a ancestral,\n",
      "En tus tierras se entrelazan, formando un carnaval.\n",
      "\n",
      "Oruro en febrero, danza de diablos y √°ngeles,\n",
      "Un sincretismo vibrante que en el alma se siente,\n",
      "Tus calles llenas de vida, colores y melod√≠as,\n",
      "Bolivia, tierra sagrada, donde el tiempo desaf√≠a.\n",
      "\n",
      "En la selva amaz√≥nica, la naturaleza respira,\n",
      "Con su fauna ex√≥tica y su flora que inspira,\n",
      "El susurro del viento entre los √°rboles altos,\n",
      "Es un himno a la vida, en este rinc√≥n tan vasto.\n",
      "\n",
      "La plata de Potos√≠, el tesoro de tu suelo,\n",
      "Cuna de riquezas y de historias sin consuelo,\n",
      "Pero en tu gente valiente, con coraz√≥n guerrero,\n",
      "Reside la esperanza de un futuro verdadero.\n",
      "\n",
      "Santa Cruz bulliciosa, con su gente tan c√°lida,\n",
      "Donde el sol se despide y la luna se instala,\n",
      "Tus mercados vibrantes, llenos de frutas y vida,\n",
      "Son un reflejo fiel de tu esencia compartida.\n",
      "\n",
      "Bolivia, patria amada, de mil rostros y paisajes,\n",
      "Con tu alma indomable y tus sue√±os salvajes,\n",
      "Eres un canto eterno, un poema sin final,\n",
      "En cada rinc√≥n tuyo, se encuentra lo esencial."
     ]
    }
   ],
   "source": [
    "response = llm.stream(\"Escribe un poema sobre de Bolivia\") # Steam responde en pedasos (chunks) separados con lo que pidamos\n",
    "for chunk in response:\n",
    "    print(chunk.content,end=\"\", flush = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cb489f",
   "metadata": {},
   "source": [
    "### TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4fe07d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬°Claro! Aqu√≠ tienes una broma sobre gallinas:\n",
      "\n",
      "¬øPor qu√© la gallina fue al banco?\n",
      "\n",
      "¬°Porque quer√≠a poner sus huevos a inter√©s! üêîüí∞\n"
     ]
    }
   ],
   "source": [
    "# ejemplo antes de usar el template\n",
    "response = llm.invoke(\"Cuenta una broma sobre gallinas\") # con temaplte haremos que gallina sea una replazada por una variable\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef75fbaa",
   "metadata": {},
   "source": [
    "#### TEMPLATE USANDO .FROM_TEMAPLATE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d5e5619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬°Claro! Aqu√≠ tienes una broma sobre perros:\n",
      "\n",
      "¬øPor qu√© los perros no usan tel√©fonos m√≥viles?\n",
      "\n",
      "¬°Porque ya tienen un gran \"ladrido\"!\n"
     ]
    }
   ],
   "source": [
    "# ejemplo 1 usando temaplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# Aqui creamos el template\n",
    "prompt = ChatPromptTemplate.from_template(\"Cuenta una broma sobre {sujeto}\")\n",
    "# ahora creamos la cadena (\"LLM chain\")\n",
    "chain = prompt | llm # hacemos que el pormpt sea pasado a la instacia LLM\n",
    "\n",
    "# pasamos el promot o instruccion usando el temaplate\n",
    "response = chain.invoke({\"sujeto\":\"perros\"}) #ojo cambiamos LLM por CHAIN y con el diccionario remplazamos gallina por perros\n",
    "print(response.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56401b51",
   "metadata": {},
   "source": [
    "#### TEMPLATE USANDO .FROM_MESSAGES()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07b5c54b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬°Por supuesto! Aqu√≠ tienes una receta deliciosa y sofisticada donde el durazno es el protagonista: **Tarta de Durazno y Crema de Almendras**.\n",
      "\n",
      "### Ingredientes\n",
      "\n",
      "#### Para la masa:\n",
      "- 1 ¬Ω tazas de harina de trigo\n",
      "- 1/4 taza de az√∫car\n",
      "- 1/4 cucharadita de sal\n",
      "- 1/2 taza de mantequilla fr√≠a, cortada en cubos\n",
      "- 1 huevo grande\n",
      "- 1-2 cucharadas de agua fr√≠a\n",
      "\n",
      "#### Para la crema de almendras:\n",
      "- 1/2 taza de mantequilla, a temperatura ambiente\n",
      "- 1/2 taza de az√∫car\n",
      "- 1 taza de almendras molidas (harina de almendra)\n",
      "- 2 huevos grandes\n",
      "- 1 cucharadita de extracto de vainilla\n",
      "- 1 cucharada de harina de trigo\n",
      "\n",
      "#### Para el relleno de durazno:\n",
      "- 4-5 duraznos maduros, pelados, deshuesados y cortados en gajos\n",
      "- 2 cucharadas de az√∫car\n",
      "- 1 cucharada de jugo de lim√≥n\n",
      "- 1 cucharadita de canela en polvo (opcional)\n",
      "\n",
      "### Instrucciones\n",
      "\n",
      "#### Preparaci√≥n de la masa:\n",
      "1. **Precalentar el horno**: Precalienta tu horno a 180¬∞C (350¬∞F).\n",
      "2. **Mezclar ingredientes secos**: En un bol grande, mezcla la harina, el az√∫car y la sal.\n",
      "3. **A√±adir mantequilla**: A√±ade la mantequilla fr√≠a y mezcla con un cortador de masa o con las manos hasta que la mezcla tenga una textura arenosa.\n",
      "4. **Incorporar huevo y agua**: Agrega el huevo y mezcla hasta que la masa comience a unirse. Si es necesario, a√±ade 1-2 cucharadas de agua fr√≠a para ayudar a unir la masa.\n",
      "5. **Refrigerar**: Forma un disco con la masa, envu√©lvelo en papel film y refrig√©ralo durante al menos 30 minutos.\n",
      "\n",
      "#### Preparaci√≥n de la crema de almendras:\n",
      "1. **Batir mantequilla y az√∫car**: En un bol, bate la mantequilla y el az√∫car hasta obtener una mezcla cremosa.\n",
      "2. **A√±adir ingredientes restantes**: Incorpora las almendras molidas, los huevos, el extracto de vainilla y la cucharada de harina. Mezcla hasta obtener una crema homog√©nea.\n",
      "\n",
      "#### Montaje de la tarta:\n",
      "1. **Extender la masa**: Saca la masa del refrigerador y exti√©ndela sobre una superficie ligeramente enharinada. Forra un molde para tarta con la masa y recorta el exceso de los bordes.\n",
      "2. **Agregar la crema de almendras**: Vierte la crema de almendras sobre la base de la tarta y distrib√∫yela de manera uniforme.\n",
      "3. **Preparar los duraznos**: En un bol, mezcla los gajos de durazno con el az√∫car, el jugo de lim√≥n y la canela (si est√°s usando).\n",
      "4. **Colocar los duraznos**: Coloca los gajos de durazno sobre la crema de almendras, organiz√°ndolos de manera decorativa o simplemente distribuy√©ndolos uniformemente.\n",
      "\n",
      "#### Horneado:\n",
      "1. **Hornear**: Coloca la tarta en el horno precalentado y hornea durante 35-40 minutos, o hasta que la masa est√© dorada y la crema de almendras est√© firme.\n",
      "2. **Enfriar**: Deja enfriar la tarta antes de desmoldarla y servirla.\n",
      "\n",
      "### Presentaci√≥n\n",
      "Sirve la tarta a temperatura ambiente o ligeramente tibia. Puedes acompa√±arla con una bola de helado de vainilla o una cucharada de crema batida para un toque extra de indulgencia.\n",
      "\n",
      "¬°Espero que disfrutes preparando y degustando esta exquisita tarta de durazno y crema de almendras!\n"
     ]
    }
   ],
   "source": [
    "# ejemplo 2 usando temaplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# Aqui creamos el template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"Eres un cocinero de renombre, crea una receta con el siguiente ingrediente principal\"),\n",
    "    (\"human\", \"{ingrediente}\")\n",
    "])\n",
    "# ahora creamos la cadena (\"LLM chain\")\n",
    "chain = prompt | llm # hacemos que el pormpt sea pasado a la instacia LLM\n",
    "\n",
    "# pasamos el promot o instruccion usando el temaplate\n",
    "response = chain.invoke({\"ingrediente\":\"durazno\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "223ba77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "melocot√≥n, pav√≠a, alb√©rchigo\n"
     ]
    }
   ],
   "source": [
    "# ejemplo 3 usando temaplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# Aqui creamos el template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"Crea una lista de tres sinoninos de la siguiente palabra. El resultado debe ser palabras separadas con coma CSV\"),\n",
    "    (\"human\", \"{ingrediente}\")\n",
    "])\n",
    "# ahora creamos la cadena (\"LLM chain\")\n",
    "chain = prompt | llm # hacemos que el pormpt sea pasado a la instacia LLM\n",
    "\n",
    "# pasamos el promot o instruccion usando el temaplate\n",
    "response = chain.invoke({\"ingrediente\":\"durazno\"})\n",
    "print(response.content) # vemos que el resulatdo es string y no una lsita pythonera"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e931eab",
   "metadata": {},
   "source": [
    "### OUTPUT PARSER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cea1d1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "melocot√≥n, albaricoque, pav√≠a\n"
     ]
    }
   ],
   "source": [
    "# ejemplo 4 usando temaplate y usandoo StrOutputParser para convertir el typo STR\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "\n",
    "def call_string_output_parser():\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\",\"Crea una lista de tres sinoninos de la siguiente palabra. El resultado debe ser palabras separadas con coma CSV\"),\n",
    "        (\"human\", \"{ingrediente}\")\n",
    "    ])\n",
    "\n",
    "    parser = StrOutputParser()\n",
    "    chain = prompt | llm | parser\n",
    "    \n",
    "    return chain.invoke({\"ingrediente\":\"durazno\"})\n",
    "\n",
    "\n",
    "print(call_string_output_parser()) # vemos que el resulatdo es string y no una lsita pythonera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2339c70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['melocot√≥n', 'albericoque', 'pav√≠a']\n"
     ]
    }
   ],
   "source": [
    "# ejemplo 5 usando temaplate y usando CommaSeparatedListOutputParser para convertir en LIST el resultado\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "\n",
    "\n",
    "def call_list_output_parser():\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\",\"Crea una lista de tres sinonimos de la siguiente palabra. El resultado debe ser palabras separadas con coma\"),\n",
    "        (\"human\", \"{ingrediente}\")\n",
    "    ])\n",
    "\n",
    "    parser = CommaSeparatedListOutputParser()\n",
    "    chain = prompt | llm | parser\n",
    "    \n",
    "    return chain.invoke({\"ingrediente\":\"durazno\"})\n",
    "\n",
    "\n",
    "print(call_list_output_parser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86d1ddb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'melocot√≥n': 'Espa√±a', 'pav√≠a': 'M√©xico', 'albericoque': 'Argentina'}\n"
     ]
    }
   ],
   "source": [
    "# ejemplo 5 usando temaplate y usando CommaSeparatedListOutputParser para convertir en LIST el resultado\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "def call_json_output_parser():\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\",\"Crea una lista de tres sinonimos de la siguiente palabra.Indicando para cada sinonimo el pais donde mas se utiliza ese sinonimo \\nFormating Instructions:{format_instructions}\"),\n",
    "        (\"human\", \"{ingrediente}\")\n",
    "    ])\n",
    "    \n",
    "    class origen(BaseModel):\n",
    "        sinonimo: str = Field(description = \"La palabra sinonimo\")\n",
    "        pais: str = Field(description = \"El pais donde mas se usa la palbra sinonimo\")\n",
    "    \n",
    "    parser = JsonOutputParser(pydantin_object = origen)\n",
    "    chain = prompt | llm | parser\n",
    "    \n",
    "    return chain.invoke({\"ingrediente\":\"durazno\", \"format_instructions\": parser.get_format_instructions()})\n",
    "\n",
    "\n",
    "print(call_json_output_parser())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dba831",
   "metadata": {},
   "source": [
    "# Conectando con fuentes externas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1955e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCEL, o LangChain Expression Language, es un lenguaje declarativo dise√±ado para facilitar la creaci√≥n y puesta en producci√≥n de cadenas de componentes LangChain. Permite a los desarrolladores encadenar componentes de manera eficiente, desde simples combinaciones de \"mensaje + LLM\" hasta cadenas complejas con cientos de pasos. LCEL est√° concebido para que los prototipos puedan pasar a producci√≥n sin necesidad de realizar cambios en el c√≥digo, lo que agiliza el proceso de desarrollo y despliegue de aplicaciones basadas en LangChain.\n"
     ]
    }
   ],
   "source": [
    "#ejemplo 1 poniendo el contexto manulmente\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "llm = ChatOpenAI(model = \"gpt-4o\",\n",
    "                 temperature = 0.4, # mas cerca de 0 mas concreto mas cerca de 1 mas creativo\n",
    "                 verbose = True                \n",
    "                )\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\" Responde a las preguntas del ususario:\n",
    "Contexto: LangChain Expression Language, o LCEL, es una forma declarativa de encadenar componentes LangChain. \n",
    "LCEL se dise√±√≥ desde el d√≠a 1 para permitir la puesta en producci√≥n de prototipos, sin cambios de c√≥digo, \n",
    "desde la cadena m√°s simple de ‚Äúmensaje + LLM‚Äù hasta las cadenas m√°s complejas \n",
    "(hemos visto a personas ejecutar con √©xito cadenas LCEL con cientos de pasos en producci√≥n).\n",
    "\n",
    "Pregunta: {input}\n",
    "\"\"\")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "response = chain.invoke({\n",
    "    \"input\":\"Que es LCEL\"\n",
    "    \n",
    "})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6a3a304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCEL, o LangChain Expression Language, es una forma declarativa de encadenar componentes LangChain. Fue dise√±ado desde el principio para permitir la puesta en producci√≥n de prototipos sin necesidad de realizar cambios en el c√≥digo. Esto aplica tanto a cadenas simples como \"mensaje + LLM\" hasta cadenas m√°s complejas, incluyendo aquellas con cientos de pasos en producci√≥n.\n"
     ]
    }
   ],
   "source": [
    "# ejemplo 2 usando \"import document\" con un solo documento docA\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.documents import Document \n",
    "\n",
    "docA = Document(\n",
    "    page_content = \"\"\"LangChain Expression Language, o LCEL, es una forma declarativa de encadenar componentes LangChain. \n",
    "LCEL se dise√±√≥ desde el d√≠a 1 para permitir la puesta en producci√≥n de prototipos, sin cambios de c√≥digo, \n",
    "desde la cadena m√°s simple de ‚Äúmensaje + LLM‚Äù hasta las cadenas m√°s complejas \n",
    "(hemos visto a personas ejecutar con √©xito cadenas LCEL con cientos de pasos en producci√≥n). \"\"\"\n",
    "\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model = \"gpt-4o\",\n",
    "                 temperature = 0.4, # mas cerca de 0 mas concreto mas cerca de 1 mas creativo\n",
    "                 verbose = True                \n",
    "                )\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\" Responde a las preguntas del ususario:\n",
    "                                            Contexto: {context}\n",
    "                                            Pregunta: {input}\n",
    "                                            \"\"\")\n",
    "\n",
    "chain = prompt | llm\n",
    "    \n",
    "response = chain.invoke({\n",
    "                            \"input\":\"Que es LCEL\",\n",
    "                            \"context\" :[docA]\n",
    "                            })\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ed11b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCEL, o LangChain Expression Language, es una forma declarativa de encadenar componentes de LangChain. Se dise√±√≥ para facilitar la puesta en producci√≥n de prototipos sin necesidad de realizar cambios en el c√≥digo. LCEL permite crear desde las cadenas m√°s simples, como \"mensaje + LLM\" (Modelo de Lenguaje de Gran Escala), hasta las cadenas m√°s complejas, con cientos de pasos, y ejecutarlas con √©xito en un entorno de producci√≥n.\n"
     ]
    }
   ],
   "source": [
    "# ejemplo 3 usando \"import document\" con varios documentos y usando create_stuf_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.documents import Document \n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "\n",
    "\n",
    "docA = Document(\n",
    "    page_content = \"\"\"LangChain Expression Language, o LCEL, es una forma declarativa de encadenar componentes LangChain. \n",
    "LCEL se dise√±√≥ desde el d√≠a 1 para permitir la puesta en producci√≥n de prototipos, sin cambios de c√≥digo, \n",
    "desde la cadena m√°s simple de ‚Äúmensaje + LLM‚Äù hasta las cadenas m√°s complejas \n",
    "(hemos visto a personas ejecutar con √©xito cadenas LCEL con cientos de pasos en producci√≥n). \"\"\"\n",
    "\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model = \"gpt-4o\",\n",
    "                 temperature = 0.4, # mas cerca de 0 mas concreto mas cerca de 1 mas creativo\n",
    "                 verbose = True                \n",
    "                )\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\" Responde a las preguntas del ususario:\n",
    "                                            Contexto: {context} \n",
    "                                            Pregunta: {input}\n",
    "                                            \"\"\")\n",
    "\n",
    "#context es mejor que se escriba en ingles\n",
    "# chain = prompt | llm \n",
    "\n",
    "chain = create_stuff_documents_chain(llm = llm, prompt = prompt)\n",
    "    \n",
    "response = chain.invoke({ \"input\":\"Que es LCEL\",\"context\" :[docA]})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189139ec",
   "metadata": {},
   "source": [
    "### EXTRAYENDO INFORMACION PARA CONTEXTO DE UNA PAGINA EXTERNA-->  ESTA ES LA PARTE LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b3eb26f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "LCEL, o **LangChain Expression Language**, es un lenguaje declarativo dise√±ado para encadenar componentes de LangChain. LCEL fue creado con el objetivo de facilitar la transici√≥n de prototipos a producci√≥n sin necesidad de cambios en el c√≥digo, desde las cadenas m√°s simples hasta las m√°s complejas. Aqu√≠ hay algunas caracter√≠sticas clave de LCEL:\n",
      "\n",
      "1. **Soporte de Streaming de Primera Clase**: LCEL permite el mejor tiempo posible hasta el primer token (time-to-first-token), lo que significa que puedes obtener fragmentos de salida de manera incremental a medida que el proveedor de LLM genera los tokens.\n",
      "\n",
      "2. **Soporte As√≠ncrono**: Las cadenas construidas con LCEL pueden ser llamadas tanto con la API sincr√≥nica como con la API as√≠ncrona, lo que permite un excelente rendimiento y la capacidad de manejar muchas solicitudes concurrentes en el mismo servidor.\n",
      "\n",
      "3. **Ejecuci√≥n Paralela Optimizada**: LCEL ejecuta autom√°ticamente pasos en paralelo cuando es posible, tanto en interfaces sincr√≥nicas como as√≠ncronas, para minimizar la latencia.\n",
      "\n",
      "4. **Reintentos y Fallbacks**: Puedes configurar reintentos y fallbacks para cualquier parte de tu cadena LCEL, mejorando la fiabilidad a escala.\n",
      "\n",
      "5. **Acceso a Resultados Intermedios**: En cadenas m√°s complejas, es √∫til acceder a los resultados de pasos intermedios antes de que se produzca la salida final. LCEL permite transmitir resultados intermedios y esto est√° disponible en cada servidor LangServe.\n",
      "\n",
      "6. **Esquemas de Entrada y Salida**: Las cadenas LCEL tienen esquemas de entrada y salida inferidos autom√°ticamente a partir de la estructura de la cadena, lo que se puede usar para la validaci√≥n de entradas y salidas.\n",
      "\n",
      "7. **Trazabilidad sin Problemas con LangSmith**: Todos los pasos en las cadenas LCEL se registran autom√°ticamente en LangSmith para una m√°xima observabilidad y capacidad de depuraci√≥n.\n",
      "\n",
      "8. **Despliegue sin Problemas con LangServe**: Cualquier cadena creada con LCEL se puede desplegar f√°cilmente utilizando LangServe.\n",
      "\n",
      "LCEL es una herramienta poderosa para desarrollar y gestionar aplicaciones de LLM complejas, proporcionando una infraestructura robusta y flexible para manejar m√∫ltiples pasos y componentes de manera eficiente.\n"
     ]
    }
   ],
   "source": [
    "# esto sirbve para cargar una pagina entera de informacion (pero ojo no estamos chunkenisando y sale caro)\n",
    "# ejemplo 4 usando \"import document\" con varios documentos y usando create_stuf_documents_chain y webbsaseloader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "\n",
    "def get_documents_from_web(url):\n",
    "    loader = WebBaseLoader(url)\n",
    "    docs = loader.load() \n",
    "    print(type(docs))\n",
    "    return docs\n",
    "\n",
    "docs = get_documents_from_web(\"https://python.langchain.com/v0.2/docs/concepts/#langchain-expression-language\")\n",
    "\n",
    "llm = ChatOpenAI(model = \"gpt-4o\",\n",
    "                 temperature = 0.4, # mas cerca de 0 mas concreto mas cerca de 1 mas creativo\n",
    "                 verbose = True                \n",
    "                )\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\" Responde a las preguntas del ususario:\n",
    "                                            Contexto: {context} \n",
    "                                            Pregunta: {input}\n",
    "                                            \"\"\")\n",
    "\n",
    "#context es mejor que se escriba en ingles\n",
    "# chain = prompt | llm \n",
    "\n",
    "chain = create_stuff_documents_chain(llm = llm, prompt = prompt)\n",
    "    \n",
    "response = chain.invoke({ \"input\":\"Que es LCEL\",\"context\" :docs})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed6398b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6883e39",
   "metadata": {},
   "source": [
    "### CHUNKENISANDO TEXTOS GANDES --> ESTA ES LA PARTE TRANSFROM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1773e688",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263\n",
      "LCEL, o LangChain Expression Language, es un lenguaje declarativo dise√±ado para encadenar componentes de LangChain. Fue creado para facilitar la transici√≥n de prototipos a producci√≥n sin necesidad de cambios en el c√≥digo, desde las cadenas m√°s simples hasta las m√°s complejas. Aqu√≠ hay algunas caracter√≠sticas clave de LCEL:\n",
      "\n",
      "1. **Soporte de Streaming de Primera Clase**: LCEL optimiza el tiempo hasta el primer token, permitiendo que los tokens se transmitan directamente desde un LLM (Modelo de Lenguaje de Gran Escala) a un analizador de salida en tiempo real.\n",
      "\n",
      "2. **Soporte As√≠ncrono**: Las cadenas construidas con LCEL pueden ser llamadas tanto con la API sincr√≥nica como con la API as√≠ncrona, lo que permite un rendimiento excelente y la capacidad de manejar muchas solicitudes concurrentes en el mismo servidor.\n",
      "\n",
      "3. **Ejecuci√≥n Paralela Optimizada**: LCEL ejecuta pasos en paralelo cuando es posible para minimizar la latencia.\n",
      "\n",
      "4. **Reintentos y Fallbacks**: Puedes configurar reintentos y fallbacks para cualquier parte de tu cadena LCEL, lo que mejora la fiabilidad a escala.\n",
      "\n",
      "5. **Acceso a Resultados Intermedios**: LCEL permite acceder a los resultados de pasos intermedios antes de que se produzca la salida final, √∫til para informar a los usuarios o depurar la cadena.\n",
      "\n",
      "6. **Esquemas de Entrada y Salida**: LCEL genera autom√°ticamente esquemas Pydantic y JSONSchema para la validaci√≥n de entradas y salidas.\n",
      "\n",
      "7. **Trazabilidad con LangSmith**: Todas las etapas de las cadenas LCEL se registran autom√°ticamente en LangSmith para una m√°xima observabilidad y capacidad de depuraci√≥n.\n",
      "\n",
      "8. **Despliegue Sencillo con LangServe**: Cualquier cadena creada con LCEL se puede desplegar f√°cilmente usando LangServe.\n",
      "\n",
      "En resumen, LCEL proporciona una forma robusta y eficiente de construir y desplegar aplicaciones complejas de LangChain, con soporte para streaming, ejecuci√≥n paralela, y trazabilidad avanzada.\n"
     ]
    }
   ],
   "source": [
    "#ahora si chunkenisarermos con text splitter\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "def get_documents_from_web(url):\n",
    "    loader = WebBaseLoader(url)\n",
    "    docs = loader.load() \n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size = 200, chunk_overlap = 20)\n",
    "    splitDocs = splitter.split_documents(docs)\n",
    "    print(len(splitDocs))\n",
    "    return splitDocs\n",
    "\n",
    "docs = get_documents_from_web(\"https://python.langchain.com/v0.2/docs/concepts/#langchain-expression-language\")\n",
    "\n",
    "llm = ChatOpenAI(model = \"gpt-4o\",\n",
    "                 temperature = 0.4, # mas cerca de 0 mas concreto mas cerca de 1 mas creativo\n",
    "                 verbose = True                \n",
    "                )\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\" Responde a las preguntas del ususario:\n",
    "                                            Contexto: {context} \n",
    "                                            Pregunta: {input}\n",
    "                                            \"\"\")\n",
    "\n",
    "#context es mejor que se escriba en ingles\n",
    "# chain = prompt | llm \n",
    "\n",
    "chain = create_stuff_documents_chain(llm = llm, prompt = prompt)\n",
    "    \n",
    "response = chain.invoke({ \"input\":\"Que es LCEL\",\"context\" :docs})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5a95ac",
   "metadata": {},
   "source": [
    "### ESTA ES LA PARTE EMBEED (VECTORIZANDO) Y CARGANDO A UNA BASE DE DATOS CREADA EN LOCAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da3f436e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263\n",
      "LCEL, o LangChain Expression Language, es un lenguaje declarativo dise√±ado para encadenar componentes de LangChain de manera eficiente. Fue creado con el prop√≥sito de facilitar la transici√≥n de prototipos a producci√≥n sin necesidad de cambios en el c√≥digo. LCEL soporta desde las cadenas m√°s simples, como una cadena de \"prompt + LLM\", hasta las m√°s complejas que pueden tener cientos de pasos. Algunas de las caracter√≠sticas clave de LCEL incluyen:\n",
      "\n",
      "1. **Soporte de streaming de primera clase**: Permite obtener el tiempo m√°s r√°pido posible hasta el primer token de salida. Por ejemplo, puede transmitir tokens directamente desde un LLM a un analizador de salida en streaming, devolviendo fragmentos de salida analizados de manera incremental al mismo ritmo que el proveedor de LLM emite los tokens.\n",
      "\n",
      "2. **Soporte as√≠ncrono**: Las cadenas construidas con LCEL pueden ser llamadas tanto con la API s√≠ncrona como con la API as√≠ncrona, permitiendo el uso del mismo c√≥digo para prototipos y producci√≥n con gran rendimiento y capacidad para manejar m√∫ltiples solicitudes concurrentes.\n",
      "\n",
      "3. **Ejecuci√≥n paralela optimizada**: LCEL ejecuta autom√°ticamente los pasos que pueden ser realizados en paralelo, tanto en interfaces s√≠ncronas como as√≠ncronas, para minimizar la latencia.\n",
      "\n",
      "4. **Reintentos y alternativas**: Permite configurar reintentos y alternativas para cualquier parte de la cadena LCEL, mejorando la fiabilidad a escala.\n",
      "\n",
      "5. **Acceso a resultados intermedios**: Es posible acceder a los resultados de pasos intermedios antes de que se produzca la salida final, √∫til para informar a los usuarios finales o para depurar la cadena.\n",
      "\n",
      "6. **Esquemas de entrada y salida**: LCEL genera autom√°ticamente esquemas Pydantic y JSONSchema a partir de la estructura de la cadena, √∫tiles para la validaci√≥n de entradas y salidas.\n",
      "\n",
      "7. **Integraci√≥n sin problemas con LangSmith**: Todos los pasos se registran autom√°ticamente en LangSmith para una m√°xima observabilidad y capacidad de depuraci√≥n.\n",
      "\n",
      "8. **Despliegue sin problemas con LangServe**: Cualquier cadena creada con LCEL puede ser f√°cilmente desplegada usando LangServe.\n",
      "\n",
      "Estas caracter√≠sticas hacen de LCEL una herramienta poderosa para desarrollar y gestionar aplicaciones complejas basadas en modelos de lenguaje.\n"
     ]
    }
   ],
   "source": [
    "# en el ejemplo pasado tengo 263 documentitos, y lo que quiero es NO pasarlos todos sino solo los mas relevantes\n",
    "#para eso se vectorizara cada chunk y se cargara de foamr separada con OpenAIEmbeddings\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "\n",
    "def get_documents_from_web(url):\n",
    "    loader = WebBaseLoader(url)\n",
    "    docs = loader.load() \n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size = 200, chunk_overlap = 20)\n",
    "    splitDocs = splitter.split_documents(docs)\n",
    "    print(len(splitDocs))\n",
    "    return splitDocs\n",
    "\n",
    "\n",
    "\n",
    "def create_db(docs):\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorStore = FAISS.from_documents(docs, embedding = embeddings)\n",
    "    return vectorStore\n",
    "\n",
    "docs = get_documents_from_web(\"https://python.langchain.com/v0.2/docs/concepts/#langchain-expression-language\")\n",
    "vectorStore = create_db(docs)\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model = \"gpt-4o\",\n",
    "                 temperature = 0.4, # mas cerca de 0 mas concreto mas cerca de 1 mas creativo\n",
    "                 verbose = True                \n",
    "                )\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\" Responde a las preguntas del ususario:\n",
    "                                            Contexto: {context} \n",
    "                                            Pregunta: {input}\n",
    "                                            \"\"\")\n",
    "\n",
    "#context es mejor que se escriba en ingles\n",
    "# chain = prompt | llm \n",
    "\n",
    "chain = create_stuff_documents_chain(llm = llm, prompt = prompt)\n",
    "    \n",
    "response = chain.invoke({ \"input\":\"Que es LCEL\",\"context\" :docs})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c1a142",
   "metadata": {},
   "source": [
    "### PARTE RETRIVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc2ac032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263\n",
      "LCEL, o LangChain Expression Language, es una plataforma declarativa dise√±ada para encadenar aplicaciones de modelos de lenguaje (LLM). Fue creada con el objetivo de facilitar la transici√≥n de prototipos a producci√≥n sin necesidad de cambiar el c√≥digo. LCEL permite construir desde las cadenas m√°s simples, como \"prompt + LLM\", hasta las m√°s complejas, con cientos de pasos, y ha demostrado ser efectiva en entornos de producci√≥n.\n",
      "\n",
      "Algunas de las caracter√≠sticas destacadas de LCEL incluyen:\n",
      "\n",
      "1. **Soporte de streaming de primera clase**: Todos los pasos en una cadena son autom√°ticamente registrados en LangSmith, lo que maximiza la capacidad de observaci√≥n y depuraci√≥n.\n",
      "2. **Despliegue sin problemas con LangServe**: Facilita el despliegue de las aplicaciones desarrolladas.\n",
      "\n",
      "En resumen, LCEL es una herramienta poderosa para desarrollar, probar, evaluar y monitorear aplicaciones de modelos de lenguaje, permitiendo una integraci√≥n y despliegue eficientes en entornos de producci√≥n.\n"
     ]
    }
   ],
   "source": [
    "#ya se carg√≥ los vectores y se creo una basew da ddatos d eventores ahora toca poder encontrar los mas relevantes a mi pregunta\n",
    "# Lo que hace es carga todo los documentos, los separa en chuca, despeus cada chuk es vectorizado \n",
    "# y se gunarad en la base de datos, despues buca los chunks mas similares a la pregunta y esos los pasa como contexto\n",
    "# de sta forma se ahorra en el costo de tokens y no se sobrepasa los limites\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "def get_documents_from_web(url):\n",
    "    loader = WebBaseLoader(url)\n",
    "    docs = loader.load() \n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size = 200, chunk_overlap = 20)\n",
    "    splitDocs = splitter.split_documents(docs)\n",
    "    print(len(splitDocs))\n",
    "    return splitDocs\n",
    "\n",
    "\n",
    "\n",
    "def create_db(docs):\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorStore = FAISS.from_documents(docs, embedding = embeddings)\n",
    "    return vectorStore\n",
    "\n",
    "\n",
    "def create_chain(vectorStore):\n",
    "    llm = ChatOpenAI(model = \"gpt-4o\",\n",
    "                     temperature = 0.4, # mas cerca de 0 mas concreto mas cerca de 1 mas creativo\n",
    "                     verbose = True                \n",
    "                    )\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(\"\"\" Responde a las preguntas del ususario:\n",
    "                                                Contexto: {context} \n",
    "                                                Pregunta: {input}\n",
    "                                                \"\"\")\n",
    "    chain = create_stuff_documents_chain(llm = llm, prompt = prompt)\n",
    "    \n",
    "    retriever = vectorStore.as_retriever()\n",
    "    retrieval_chain = create_retrieval_chain(retriever,chain )\n",
    "    return retrieval_chain\n",
    "\n",
    "\n",
    "docs = get_documents_from_web(\"https://python.langchain.com/v0.2/docs/concepts/#langchain-expression-language\")\n",
    "vectorStore = create_db(docs)\n",
    "chain = create_chain(vectorStore)\n",
    "    \n",
    "response = chain.invoke({ \"input\":\"Que es LCEL\"})# \"context\" :docs --> sacamos\n",
    "\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd2528a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae493b88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81379692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bad9363",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
