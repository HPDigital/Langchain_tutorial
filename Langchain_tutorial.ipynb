{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6f94c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a079240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc4f1c8",
   "metadata": {},
   "source": [
    "### INSTANCIANDO EL MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06faedd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model = \"gpt-4o\",\n",
    "                 temperature = 0.7, # mas cerca de 0 mas concreto mas cerca de 1 mas creativo\n",
    "                 verbose = True                \n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c95c00",
   "metadata": {},
   "source": [
    "### INVOKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8dce642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Â¡Hola! Estoy bien, gracias. Â¿Y tÃº? Â¿En quÃ© puedo ayudarte hoy?\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"Hola, como estas?\") # invoke solo acepta una pregunta\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e59bb7f",
   "metadata": {},
   "source": [
    "### BATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "077dcef6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Â¡Hola! Estoy aquÃ­ para ayudarte. Â¿En quÃ© puedo asistirte hoy?\n",
      "**************************\n",
      "En el corazÃ³n de SudamÃ©rica, un tesoro escondido,\n",
      "Bolivia, tierra de alturas y misterios compartidos,\n",
      "Donde el cielo se encuentra con la tierra en un abrazo sin fin,\n",
      "Y el alma de su gente resuena en cada rincÃ³n y confÃ­n.\n",
      "\n",
      "Desde el altiplano, donde el aire es cristalino y puro,\n",
      "Hasta las selvas rebosantes, donde los rÃ­os murmuran su canto maduro,\n",
      "El Salar de Uyuni, espejo del cielo, vasto y brillante,\n",
      "Refleja sueÃ±os y esperanzas en un paisaje deslumbrante.\n",
      "\n",
      "La Paz, ciudad de alturas, donde las montaÃ±as son guardianas,\n",
      "Mirando con ojos antiguos, historias de luchas y hazaÃ±as,\n",
      "El Illimani vigila, con su manto nevado y eterno,\n",
      "Mientras las calles vibran con vida, en un ritmo diurno y nocturno.\n",
      "\n",
      "El lago Titicaca, sagrado y sereno, cuna de leyendas,\n",
      "Sus aguas profundas guardan secretos y ofrendas,\n",
      "Islas del Sol y de la Luna, con historias de dioses y reyes,\n",
      "Donde el tiempo parece detenerse, en sus mÃ¡gicos atardeceres.\n",
      "\n",
      "Cochabamba, Santa Cruz, PotosÃ­ y Sucre, nombres que resuenan,\n",
      "Cada regiÃ³n con su esencia, cada pueblo con su pena,\n",
      "La mina de plata que alimentÃ³ imperios, la ciudad blanca y su historia,\n",
      "Bolivia, mosaico de culturas, en una danza de gloria.\n",
      "\n",
      "Tus mercados llenos de colores, tus tejidos y artesanÃ­as,\n",
      "Son el reflejo de un pueblo que en sus manos guarda vidas,\n",
      "La mÃºsica de los Andes, con charangos y zampoÃ±as,\n",
      "Canta al viento y al destino, en melodÃ­as que nos asombran.\n",
      "\n",
      "Bolivia, tierra de contrastes, de lo antiguo y lo nuevo,\n",
      "En tus valles y montaÃ±as, se entrelazan los sueÃ±os,\n",
      "Eres un canto a la resistencia, un poema de esperanza,\n",
      "Un lugar donde el pasado y el futuro se abrazan.\n",
      "\n",
      "Que tu espÃ­ritu indomable siga brillando con vigor,\n",
      "En cada rincÃ³n de tu geografÃ­a, en cada gesto de amor,\n",
      "Bolivia, joya del altiplano, joya de SudamÃ©rica,\n",
      "En tus paisajes y tu gente, encontramos nuestra AmÃ©rica.\n"
     ]
    }
   ],
   "source": [
    "response = llm.batch([\"Hola, como estas?\", \"Escribe un poema sobre de Bolivia\"])\n",
    "print(response[0].content)  # batch acepta varias preguntas a la vez y las corre en paralelo\n",
    "print(\"**************************\")\n",
    "print(response[1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647fecfe",
   "metadata": {},
   "source": [
    "### STREAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d685d4d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En el corazÃ³n de SudamÃ©rica, allÃ­ se encuentra,\n",
      "Una tierra de montaÃ±as, selvas y paz eterna,\n",
      "Bolivia, joya andina, con tu alma tan profunda,\n",
      "De la Pachamama eres hija, con historia fecunda.\n",
      "\n",
      "Desde el altiplano donde el cÃ³ndor vuela alto,\n",
      "Hasta los valles verdes donde el rÃ­o canta bajo,\n",
      "Tu diversidad es vasta, tu cultura es un tesoro,\n",
      "Un crisol de tradiciones que en tus venas atesoro.\n",
      "\n",
      "En el titÃ¡nico Titicaca, espejo del cielo azul,\n",
      "Navegan leyendas antiguas, de un pueblo que es fiel,\n",
      "Aymaras y quechuas, con sabidurÃ­a ancestral,\n",
      "En tus tierras se entrelazan, formando un carnaval.\n",
      "\n",
      "Oruro en febrero, danza de diablos y Ã¡ngeles,\n",
      "Un sincretismo vibrante que en el alma se siente,\n",
      "Tus calles llenas de vida, colores y melodÃ­as,\n",
      "Bolivia, tierra sagrada, donde el tiempo desafÃ­a.\n",
      "\n",
      "En la selva amazÃ³nica, la naturaleza respira,\n",
      "Con su fauna exÃ³tica y su flora que inspira,\n",
      "El susurro del viento entre los Ã¡rboles altos,\n",
      "Es un himno a la vida, en este rincÃ³n tan vasto.\n",
      "\n",
      "La plata de PotosÃ­, el tesoro de tu suelo,\n",
      "Cuna de riquezas y de historias sin consuelo,\n",
      "Pero en tu gente valiente, con corazÃ³n guerrero,\n",
      "Reside la esperanza de un futuro verdadero.\n",
      "\n",
      "Santa Cruz bulliciosa, con su gente tan cÃ¡lida,\n",
      "Donde el sol se despide y la luna se instala,\n",
      "Tus mercados vibrantes, llenos de frutas y vida,\n",
      "Son un reflejo fiel de tu esencia compartida.\n",
      "\n",
      "Bolivia, patria amada, de mil rostros y paisajes,\n",
      "Con tu alma indomable y tus sueÃ±os salvajes,\n",
      "Eres un canto eterno, un poema sin final,\n",
      "En cada rincÃ³n tuyo, se encuentra lo esencial."
     ]
    }
   ],
   "source": [
    "response = llm.stream(\"Escribe un poema sobre de Bolivia\") # Steam responde en pedasos (chunks) separados con lo que pidamos\n",
    "for chunk in response:\n",
    "    print(chunk.content,end=\"\", flush = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cb489f",
   "metadata": {},
   "source": [
    "### TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4fe07d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Â¡Claro! AquÃ­ tienes una broma sobre gallinas:\n",
      "\n",
      "Â¿Por quÃ© la gallina fue al banco?\n",
      "\n",
      "Â¡Porque querÃ­a poner sus huevos a interÃ©s! ðŸ”ðŸ’°\n"
     ]
    }
   ],
   "source": [
    "# ejemplo antes de usar el template\n",
    "response = llm.invoke(\"Cuenta una broma sobre gallinas\") # con temaplte haremos que gallina sea una replazada por una variable\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef75fbaa",
   "metadata": {},
   "source": [
    "#### TEMPLATE USANDO .FROM_TEMAPLATE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d5e5619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Â¡Claro! AquÃ­ tienes una broma sobre perros:\n",
      "\n",
      "Â¿Por quÃ© los perros no usan telÃ©fonos mÃ³viles?\n",
      "\n",
      "Â¡Porque ya tienen un gran \"ladrido\"!\n"
     ]
    }
   ],
   "source": [
    "# ejemplo 1 usando temaplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# Aqui creamos el template\n",
    "prompt = ChatPromptTemplate.from_template(\"Cuenta una broma sobre {sujeto}\")\n",
    "# ahora creamos la cadena (\"LLM chain\")\n",
    "chain = prompt | llm # hacemos que el pormpt sea pasado a la instacia LLM\n",
    "\n",
    "# pasamos el promot o instruccion usando el temaplate\n",
    "response = chain.invoke({\"sujeto\":\"perros\"}) #ojo cambiamos LLM por CHAIN y con el diccionario remplazamos gallina por perros\n",
    "print(response.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56401b51",
   "metadata": {},
   "source": [
    "#### TEMPLATE USANDO .FROM_MESSAGES()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07b5c54b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Â¡Por supuesto! AquÃ­ tienes una receta deliciosa y sofisticada donde el durazno es el protagonista: **Tarta de Durazno y Crema de Almendras**.\n",
      "\n",
      "### Ingredientes\n",
      "\n",
      "#### Para la masa:\n",
      "- 1 Â½ tazas de harina de trigo\n",
      "- 1/4 taza de azÃºcar\n",
      "- 1/4 cucharadita de sal\n",
      "- 1/2 taza de mantequilla frÃ­a, cortada en cubos\n",
      "- 1 huevo grande\n",
      "- 1-2 cucharadas de agua frÃ­a\n",
      "\n",
      "#### Para la crema de almendras:\n",
      "- 1/2 taza de mantequilla, a temperatura ambiente\n",
      "- 1/2 taza de azÃºcar\n",
      "- 1 taza de almendras molidas (harina de almendra)\n",
      "- 2 huevos grandes\n",
      "- 1 cucharadita de extracto de vainilla\n",
      "- 1 cucharada de harina de trigo\n",
      "\n",
      "#### Para el relleno de durazno:\n",
      "- 4-5 duraznos maduros, pelados, deshuesados y cortados en gajos\n",
      "- 2 cucharadas de azÃºcar\n",
      "- 1 cucharada de jugo de limÃ³n\n",
      "- 1 cucharadita de canela en polvo (opcional)\n",
      "\n",
      "### Instrucciones\n",
      "\n",
      "#### PreparaciÃ³n de la masa:\n",
      "1. **Precalentar el horno**: Precalienta tu horno a 180Â°C (350Â°F).\n",
      "2. **Mezclar ingredientes secos**: En un bol grande, mezcla la harina, el azÃºcar y la sal.\n",
      "3. **AÃ±adir mantequilla**: AÃ±ade la mantequilla frÃ­a y mezcla con un cortador de masa o con las manos hasta que la mezcla tenga una textura arenosa.\n",
      "4. **Incorporar huevo y agua**: Agrega el huevo y mezcla hasta que la masa comience a unirse. Si es necesario, aÃ±ade 1-2 cucharadas de agua frÃ­a para ayudar a unir la masa.\n",
      "5. **Refrigerar**: Forma un disco con la masa, envuÃ©lvelo en papel film y refrigÃ©ralo durante al menos 30 minutos.\n",
      "\n",
      "#### PreparaciÃ³n de la crema de almendras:\n",
      "1. **Batir mantequilla y azÃºcar**: En un bol, bate la mantequilla y el azÃºcar hasta obtener una mezcla cremosa.\n",
      "2. **AÃ±adir ingredientes restantes**: Incorpora las almendras molidas, los huevos, el extracto de vainilla y la cucharada de harina. Mezcla hasta obtener una crema homogÃ©nea.\n",
      "\n",
      "#### Montaje de la tarta:\n",
      "1. **Extender la masa**: Saca la masa del refrigerador y extiÃ©ndela sobre una superficie ligeramente enharinada. Forra un molde para tarta con la masa y recorta el exceso de los bordes.\n",
      "2. **Agregar la crema de almendras**: Vierte la crema de almendras sobre la base de la tarta y distribÃºyela de manera uniforme.\n",
      "3. **Preparar los duraznos**: En un bol, mezcla los gajos de durazno con el azÃºcar, el jugo de limÃ³n y la canela (si estÃ¡s usando).\n",
      "4. **Colocar los duraznos**: Coloca los gajos de durazno sobre la crema de almendras, organizÃ¡ndolos de manera decorativa o simplemente distribuyÃ©ndolos uniformemente.\n",
      "\n",
      "#### Horneado:\n",
      "1. **Hornear**: Coloca la tarta en el horno precalentado y hornea durante 35-40 minutos, o hasta que la masa estÃ© dorada y la crema de almendras estÃ© firme.\n",
      "2. **Enfriar**: Deja enfriar la tarta antes de desmoldarla y servirla.\n",
      "\n",
      "### PresentaciÃ³n\n",
      "Sirve la tarta a temperatura ambiente o ligeramente tibia. Puedes acompaÃ±arla con una bola de helado de vainilla o una cucharada de crema batida para un toque extra de indulgencia.\n",
      "\n",
      "Â¡Espero que disfrutes preparando y degustando esta exquisita tarta de durazno y crema de almendras!\n"
     ]
    }
   ],
   "source": [
    "# ejemplo 2 usando temaplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# Aqui creamos el template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"Eres un cocinero de renombre, crea una receta con el siguiente ingrediente principal\"),\n",
    "    (\"human\", \"{ingrediente}\")\n",
    "])\n",
    "# ahora creamos la cadena (\"LLM chain\")\n",
    "chain = prompt | llm # hacemos que el pormpt sea pasado a la instacia LLM\n",
    "\n",
    "# pasamos el promot o instruccion usando el temaplate\n",
    "response = chain.invoke({\"ingrediente\":\"durazno\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "223ba77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "melocotÃ³n, pavÃ­a, albÃ©rchigo\n"
     ]
    }
   ],
   "source": [
    "# ejemplo 3 usando temaplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# Aqui creamos el template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"Crea una lista de tres sinoninos de la siguiente palabra. El resultado debe ser palabras separadas con coma CSV\"),\n",
    "    (\"human\", \"{ingrediente}\")\n",
    "])\n",
    "# ahora creamos la cadena (\"LLM chain\")\n",
    "chain = prompt | llm # hacemos que el pormpt sea pasado a la instacia LLM\n",
    "\n",
    "# pasamos el promot o instruccion usando el temaplate\n",
    "response = chain.invoke({\"ingrediente\":\"durazno\"})\n",
    "print(response.content) # vemos que el resulatdo es string y no una lsita pythonera"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e931eab",
   "metadata": {},
   "source": [
    "### OUTPUT PARSER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cea1d1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "melocotÃ³n, albaricoque, pavÃ­a\n"
     ]
    }
   ],
   "source": [
    "# ejemplo 4 usando temaplate y usandoo StrOutputParser para convertir el typo STR\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "\n",
    "def call_string_output_parser():\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\",\"Crea una lista de tres sinoninos de la siguiente palabra. El resultado debe ser palabras separadas con coma CSV\"),\n",
    "        (\"human\", \"{ingrediente}\")\n",
    "    ])\n",
    "\n",
    "    parser = StrOutputParser()\n",
    "    chain = prompt | llm | parser\n",
    "    \n",
    "    return chain.invoke({\"ingrediente\":\"durazno\"})\n",
    "\n",
    "\n",
    "print(call_string_output_parser()) # vemos que el resulatdo es string y no una lsita pythonera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2339c70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['melocotÃ³n', 'albericoque', 'pavÃ­a']\n"
     ]
    }
   ],
   "source": [
    "# ejemplo 5 usando temaplate y usando CommaSeparatedListOutputParser para convertir en LIST el resultado\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "\n",
    "\n",
    "def call_list_output_parser():\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\",\"Crea una lista de tres sinonimos de la siguiente palabra. El resultado debe ser palabras separadas con coma\"),\n",
    "        (\"human\", \"{ingrediente}\")\n",
    "    ])\n",
    "\n",
    "    parser = CommaSeparatedListOutputParser()\n",
    "    chain = prompt | llm | parser\n",
    "    \n",
    "    return chain.invoke({\"ingrediente\":\"durazno\"})\n",
    "\n",
    "\n",
    "print(call_list_output_parser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86d1ddb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'melocotÃ³n': 'EspaÃ±a', 'pavÃ­a': 'MÃ©xico', 'albericoque': 'Argentina'}\n"
     ]
    }
   ],
   "source": [
    "# ejemplo 5 usando temaplate y usando CommaSeparatedListOutputParser para convertir en LIST el resultado\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "def call_json_output_parser():\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\",\"Crea una lista de tres sinonimos de la siguiente palabra.Indicando para cada sinonimo el pais donde mas se utiliza ese sinonimo \\nFormating Instructions:{format_instructions}\"),\n",
    "        (\"human\", \"{ingrediente}\")\n",
    "    ])\n",
    "    \n",
    "    class origen(BaseModel):\n",
    "        sinonimo: str = Field(description = \"La palabra sinonimo\")\n",
    "        pais: str = Field(description = \"El pais donde mas se usa la palbra sinonimo\")\n",
    "    \n",
    "    parser = JsonOutputParser(pydantin_object = origen)\n",
    "    chain = prompt | llm | parser\n",
    "    \n",
    "    return chain.invoke({\"ingrediente\":\"durazno\", \"format_instructions\": parser.get_format_instructions()})\n",
    "\n",
    "\n",
    "print(call_json_output_parser())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dba831",
   "metadata": {},
   "source": [
    "# Conectando con fuentes externas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1955e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCEL, o LangChain Expression Language, es un lenguaje declarativo diseÃ±ado para facilitar la creaciÃ³n y puesta en producciÃ³n de cadenas de componentes LangChain. Permite a los desarrolladores encadenar componentes de manera eficiente, desde simples combinaciones de \"mensaje + LLM\" hasta cadenas complejas con cientos de pasos. LCEL estÃ¡ concebido para que los prototipos puedan pasar a producciÃ³n sin necesidad de realizar cambios en el cÃ³digo, lo que agiliza el proceso de desarrollo y despliegue de aplicaciones basadas en LangChain.\n"
     ]
    }
   ],
   "source": [
    "#ejemplo 1 poniendo el contexto manulmente\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "llm = ChatOpenAI(model = \"gpt-4o\",\n",
    "                 temperature = 0.4, # mas cerca de 0 mas concreto mas cerca de 1 mas creativo\n",
    "                 verbose = True                \n",
    "                )\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\" Responde a las preguntas del ususario:\n",
    "Contexto: LangChain Expression Language, o LCEL, es una forma declarativa de encadenar componentes LangChain. \n",
    "LCEL se diseÃ±Ã³ desde el dÃ­a 1 para permitir la puesta en producciÃ³n de prototipos, sin cambios de cÃ³digo, \n",
    "desde la cadena mÃ¡s simple de â€œmensaje + LLMâ€ hasta las cadenas mÃ¡s complejas \n",
    "(hemos visto a personas ejecutar con Ã©xito cadenas LCEL con cientos de pasos en producciÃ³n).\n",
    "\n",
    "Pregunta: {input}\n",
    "\"\"\")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "response = chain.invoke({\n",
    "    \"input\":\"Que es LCEL\"\n",
    "    \n",
    "})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6a3a304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCEL, o LangChain Expression Language, es una forma declarativa de encadenar componentes LangChain. Fue diseÃ±ado desde el principio para permitir la puesta en producciÃ³n de prototipos sin necesidad de realizar cambios en el cÃ³digo. Esto aplica tanto a cadenas simples como \"mensaje + LLM\" hasta cadenas mÃ¡s complejas, incluyendo aquellas con cientos de pasos en producciÃ³n.\n"
     ]
    }
   ],
   "source": [
    "# ejemplo 2 usando \"import document\" con un solo documento docA\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.documents import Document \n",
    "\n",
    "docA = Document(\n",
    "    page_content = \"\"\"LangChain Expression Language, o LCEL, es una forma declarativa de encadenar componentes LangChain. \n",
    "LCEL se diseÃ±Ã³ desde el dÃ­a 1 para permitir la puesta en producciÃ³n de prototipos, sin cambios de cÃ³digo, \n",
    "desde la cadena mÃ¡s simple de â€œmensaje + LLMâ€ hasta las cadenas mÃ¡s complejas \n",
    "(hemos visto a personas ejecutar con Ã©xito cadenas LCEL con cientos de pasos en producciÃ³n). \"\"\"\n",
    "\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model = \"gpt-4o\",\n",
    "                 temperature = 0.4, # mas cerca de 0 mas concreto mas cerca de 1 mas creativo\n",
    "                 verbose = True                \n",
    "                )\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\" Responde a las preguntas del ususario:\n",
    "                                            Contexto: {context}\n",
    "                                            Pregunta: {input}\n",
    "                                            \"\"\")\n",
    "\n",
    "chain = prompt | llm\n",
    "    \n",
    "response = chain.invoke({\n",
    "                            \"input\":\"Que es LCEL\",\n",
    "                            \"context\" :[docA]\n",
    "                            })\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ed11b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCEL, o LangChain Expression Language, es una forma declarativa de encadenar componentes de LangChain. Se diseÃ±Ã³ para facilitar la puesta en producciÃ³n de prototipos sin necesidad de realizar cambios en el cÃ³digo. LCEL permite crear desde las cadenas mÃ¡s simples, como \"mensaje + LLM\" (Modelo de Lenguaje de Gran Escala), hasta las cadenas mÃ¡s complejas, con cientos de pasos, y ejecutarlas con Ã©xito en un entorno de producciÃ³n.\n"
     ]
    }
   ],
   "source": [
    "# ejemplo 3 usando \"import document\" con varios documentos y usando create_stuf_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.documents import Document \n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "\n",
    "\n",
    "docA = Document(\n",
    "    page_content = \"\"\"LangChain Expression Language, o LCEL, es una forma declarativa de encadenar componentes LangChain. \n",
    "LCEL se diseÃ±Ã³ desde el dÃ­a 1 para permitir la puesta en producciÃ³n de prototipos, sin cambios de cÃ³digo, \n",
    "desde la cadena mÃ¡s simple de â€œmensaje + LLMâ€ hasta las cadenas mÃ¡s complejas \n",
    "(hemos visto a personas ejecutar con Ã©xito cadenas LCEL con cientos de pasos en producciÃ³n). \"\"\"\n",
    "\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model = \"gpt-4o\",\n",
    "                 temperature = 0.4, # mas cerca de 0 mas concreto mas cerca de 1 mas creativo\n",
    "                 verbose = True                \n",
    "                )\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\" Responde a las preguntas del ususario:\n",
    "                                            Contexto: {context} \n",
    "                                            Pregunta: {input}\n",
    "                                            \"\"\")\n",
    "\n",
    "#context es mejor que se escriba en ingles\n",
    "# chain = prompt | llm \n",
    "\n",
    "chain = create_stuff_documents_chain(llm = llm, prompt = prompt)\n",
    "    \n",
    "response = chain.invoke({ \"input\":\"Que es LCEL\",\"context\" :[docA]})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189139ec",
   "metadata": {},
   "source": [
    "### EXTRAYENDO INFORMACION PARA CONTEXTO DE UNA PAGINA EXTERNA-->  ESTA ES LA PARTE LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b3eb26f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "LCEL, o **LangChain Expression Language**, es un lenguaje declarativo diseÃ±ado para encadenar componentes de LangChain. LCEL fue creado con el objetivo de facilitar la transiciÃ³n de prototipos a producciÃ³n sin necesidad de cambios en el cÃ³digo, desde las cadenas mÃ¡s simples hasta las mÃ¡s complejas. AquÃ­ hay algunas caracterÃ­sticas clave de LCEL:\n",
      "\n",
      "1. **Soporte de Streaming de Primera Clase**: LCEL permite el mejor tiempo posible hasta el primer token (time-to-first-token), lo que significa que puedes obtener fragmentos de salida de manera incremental a medida que el proveedor de LLM genera los tokens.\n",
      "\n",
      "2. **Soporte AsÃ­ncrono**: Las cadenas construidas con LCEL pueden ser llamadas tanto con la API sincrÃ³nica como con la API asÃ­ncrona, lo que permite un excelente rendimiento y la capacidad de manejar muchas solicitudes concurrentes en el mismo servidor.\n",
      "\n",
      "3. **EjecuciÃ³n Paralela Optimizada**: LCEL ejecuta automÃ¡ticamente pasos en paralelo cuando es posible, tanto en interfaces sincrÃ³nicas como asÃ­ncronas, para minimizar la latencia.\n",
      "\n",
      "4. **Reintentos y Fallbacks**: Puedes configurar reintentos y fallbacks para cualquier parte de tu cadena LCEL, mejorando la fiabilidad a escala.\n",
      "\n",
      "5. **Acceso a Resultados Intermedios**: En cadenas mÃ¡s complejas, es Ãºtil acceder a los resultados de pasos intermedios antes de que se produzca la salida final. LCEL permite transmitir resultados intermedios y esto estÃ¡ disponible en cada servidor LangServe.\n",
      "\n",
      "6. **Esquemas de Entrada y Salida**: Las cadenas LCEL tienen esquemas de entrada y salida inferidos automÃ¡ticamente a partir de la estructura de la cadena, lo que se puede usar para la validaciÃ³n de entradas y salidas.\n",
      "\n",
      "7. **Trazabilidad sin Problemas con LangSmith**: Todos los pasos en las cadenas LCEL se registran automÃ¡ticamente en LangSmith para una mÃ¡xima observabilidad y capacidad de depuraciÃ³n.\n",
      "\n",
      "8. **Despliegue sin Problemas con LangServe**: Cualquier cadena creada con LCEL se puede desplegar fÃ¡cilmente utilizando LangServe.\n",
      "\n",
      "LCEL es una herramienta poderosa para desarrollar y gestionar aplicaciones de LLM complejas, proporcionando una infraestructura robusta y flexible para manejar mÃºltiples pasos y componentes de manera eficiente.\n"
     ]
    }
   ],
   "source": [
    "# esto sirbve para cargar una pagina entera de informacion (pero ojo no estamos chunkenisando y sale caro)\n",
    "# ejemplo 4 usando \"import document\" con varios documentos y usando create_stuf_documents_chain y webbsaseloader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "\n",
    "def get_documents_from_web(url):\n",
    "    loader = WebBaseLoader(url)\n",
    "    docs = loader.load() \n",
    "    print(type(docs))\n",
    "    return docs\n",
    "\n",
    "docs = get_documents_from_web(\"https://python.langchain.com/v0.2/docs/concepts/#langchain-expression-language\")\n",
    "\n",
    "llm = ChatOpenAI(model = \"gpt-4o\",\n",
    "                 temperature = 0.4, # mas cerca de 0 mas concreto mas cerca de 1 mas creativo\n",
    "                 verbose = True                \n",
    "                )\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\" Responde a las preguntas del ususario:\n",
    "                                            Contexto: {context} \n",
    "                                            Pregunta: {input}\n",
    "                                            \"\"\")\n",
    "\n",
    "#context es mejor que se escriba en ingles\n",
    "# chain = prompt | llm \n",
    "\n",
    "chain = create_stuff_documents_chain(llm = llm, prompt = prompt)\n",
    "    \n",
    "response = chain.invoke({ \"input\":\"Que es LCEL\",\"context\" :docs})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed6398b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6883e39",
   "metadata": {},
   "source": [
    "### CHUNKENISANDO TEXTOS GANDES --> ESTA ES LA PARTE TRANSFROM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1773e688",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263\n",
      "LCEL, o LangChain Expression Language, es un lenguaje declarativo diseÃ±ado para encadenar componentes de LangChain. Fue creado para facilitar la transiciÃ³n de prototipos a producciÃ³n sin necesidad de cambios en el cÃ³digo, desde las cadenas mÃ¡s simples hasta las mÃ¡s complejas. AquÃ­ hay algunas caracterÃ­sticas clave de LCEL:\n",
      "\n",
      "1. **Soporte de Streaming de Primera Clase**: LCEL optimiza el tiempo hasta el primer token, permitiendo que los tokens se transmitan directamente desde un LLM (Modelo de Lenguaje de Gran Escala) a un analizador de salida en tiempo real.\n",
      "\n",
      "2. **Soporte AsÃ­ncrono**: Las cadenas construidas con LCEL pueden ser llamadas tanto con la API sincrÃ³nica como con la API asÃ­ncrona, lo que permite un rendimiento excelente y la capacidad de manejar muchas solicitudes concurrentes en el mismo servidor.\n",
      "\n",
      "3. **EjecuciÃ³n Paralela Optimizada**: LCEL ejecuta pasos en paralelo cuando es posible para minimizar la latencia.\n",
      "\n",
      "4. **Reintentos y Fallbacks**: Puedes configurar reintentos y fallbacks para cualquier parte de tu cadena LCEL, lo que mejora la fiabilidad a escala.\n",
      "\n",
      "5. **Acceso a Resultados Intermedios**: LCEL permite acceder a los resultados de pasos intermedios antes de que se produzca la salida final, Ãºtil para informar a los usuarios o depurar la cadena.\n",
      "\n",
      "6. **Esquemas de Entrada y Salida**: LCEL genera automÃ¡ticamente esquemas Pydantic y JSONSchema para la validaciÃ³n de entradas y salidas.\n",
      "\n",
      "7. **Trazabilidad con LangSmith**: Todas las etapas de las cadenas LCEL se registran automÃ¡ticamente en LangSmith para una mÃ¡xima observabilidad y capacidad de depuraciÃ³n.\n",
      "\n",
      "8. **Despliegue Sencillo con LangServe**: Cualquier cadena creada con LCEL se puede desplegar fÃ¡cilmente usando LangServe.\n",
      "\n",
      "En resumen, LCEL proporciona una forma robusta y eficiente de construir y desplegar aplicaciones complejas de LangChain, con soporte para streaming, ejecuciÃ³n paralela, y trazabilidad avanzada.\n"
     ]
    }
   ],
   "source": [
    "#ahora si chunkenisarermos con text splitter\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "def get_documents_from_web(url):\n",
    "    loader = WebBaseLoader(url)\n",
    "    docs = loader.load() \n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size = 200, chunk_overlap = 20)\n",
    "    splitDocs = splitter.split_documents(docs)\n",
    "    print(len(splitDocs))\n",
    "    return splitDocs\n",
    "\n",
    "docs = get_documents_from_web(\"https://python.langchain.com/v0.2/docs/concepts/#langchain-expression-language\")\n",
    "\n",
    "llm = ChatOpenAI(model = \"gpt-4o\",\n",
    "                 temperature = 0.4, # mas cerca de 0 mas concreto mas cerca de 1 mas creativo\n",
    "                 verbose = True                \n",
    "                )\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\" Responde a las preguntas del ususario:\n",
    "                                            Contexto: {context} \n",
    "                                            Pregunta: {input}\n",
    "                                            \"\"\")\n",
    "\n",
    "#context es mejor que se escriba en ingles\n",
    "# chain = prompt | llm \n",
    "\n",
    "chain = create_stuff_documents_chain(llm = llm, prompt = prompt)\n",
    "    \n",
    "response = chain.invoke({ \"input\":\"Que es LCEL\",\"context\" :docs})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5a95ac",
   "metadata": {},
   "source": [
    "### ESTA ES LA PARTE EMBEED (VECTORIZANDO) Y CARGANDO A UNA BASE DE DATOS CREADA EN LOCAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da3f436e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263\n",
      "LCEL, o LangChain Expression Language, es un lenguaje declarativo diseÃ±ado para encadenar componentes de LangChain de manera eficiente. Fue creado con el propÃ³sito de facilitar la transiciÃ³n de prototipos a producciÃ³n sin necesidad de cambios en el cÃ³digo. LCEL soporta desde las cadenas mÃ¡s simples, como una cadena de \"prompt + LLM\", hasta las mÃ¡s complejas que pueden tener cientos de pasos. Algunas de las caracterÃ­sticas clave de LCEL incluyen:\n",
      "\n",
      "1. **Soporte de streaming de primera clase**: Permite obtener el tiempo mÃ¡s rÃ¡pido posible hasta el primer token de salida. Por ejemplo, puede transmitir tokens directamente desde un LLM a un analizador de salida en streaming, devolviendo fragmentos de salida analizados de manera incremental al mismo ritmo que el proveedor de LLM emite los tokens.\n",
      "\n",
      "2. **Soporte asÃ­ncrono**: Las cadenas construidas con LCEL pueden ser llamadas tanto con la API sÃ­ncrona como con la API asÃ­ncrona, permitiendo el uso del mismo cÃ³digo para prototipos y producciÃ³n con gran rendimiento y capacidad para manejar mÃºltiples solicitudes concurrentes.\n",
      "\n",
      "3. **EjecuciÃ³n paralela optimizada**: LCEL ejecuta automÃ¡ticamente los pasos que pueden ser realizados en paralelo, tanto en interfaces sÃ­ncronas como asÃ­ncronas, para minimizar la latencia.\n",
      "\n",
      "4. **Reintentos y alternativas**: Permite configurar reintentos y alternativas para cualquier parte de la cadena LCEL, mejorando la fiabilidad a escala.\n",
      "\n",
      "5. **Acceso a resultados intermedios**: Es posible acceder a los resultados de pasos intermedios antes de que se produzca la salida final, Ãºtil para informar a los usuarios finales o para depurar la cadena.\n",
      "\n",
      "6. **Esquemas de entrada y salida**: LCEL genera automÃ¡ticamente esquemas Pydantic y JSONSchema a partir de la estructura de la cadena, Ãºtiles para la validaciÃ³n de entradas y salidas.\n",
      "\n",
      "7. **IntegraciÃ³n sin problemas con LangSmith**: Todos los pasos se registran automÃ¡ticamente en LangSmith para una mÃ¡xima observabilidad y capacidad de depuraciÃ³n.\n",
      "\n",
      "8. **Despliegue sin problemas con LangServe**: Cualquier cadena creada con LCEL puede ser fÃ¡cilmente desplegada usando LangServe.\n",
      "\n",
      "Estas caracterÃ­sticas hacen de LCEL una herramienta poderosa para desarrollar y gestionar aplicaciones complejas basadas en modelos de lenguaje.\n"
     ]
    }
   ],
   "source": [
    "# en el ejemplo pasado tengo 263 documentitos, y lo que quiero es NO pasarlos todos sino solo los mas relevantes\n",
    "#para eso se vectorizara cada chunk y se cargara de foamr separada con OpenAIEmbeddings\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "\n",
    "def get_documents_from_web(url):\n",
    "    loader = WebBaseLoader(url)\n",
    "    docs = loader.load() \n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size = 200, chunk_overlap = 20)\n",
    "    splitDocs = splitter.split_documents(docs)\n",
    "    print(len(splitDocs))\n",
    "    return splitDocs\n",
    "\n",
    "\n",
    "\n",
    "def create_db(docs):\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorStore = FAISS.from_documents(docs, embedding = embeddings)\n",
    "    return vectorStore\n",
    "\n",
    "docs = get_documents_from_web(\"https://python.langchain.com/v0.2/docs/concepts/#langchain-expression-language\")\n",
    "vectorStore = create_db(docs)\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model = \"gpt-4o\",\n",
    "                 temperature = 0.4, # mas cerca de 0 mas concreto mas cerca de 1 mas creativo\n",
    "                 verbose = True                \n",
    "                )\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\" Responde a las preguntas del ususario:\n",
    "                                            Contexto: {context} \n",
    "                                            Pregunta: {input}\n",
    "                                            \"\"\")\n",
    "\n",
    "#context es mejor que se escriba en ingles\n",
    "# chain = prompt | llm \n",
    "\n",
    "chain = create_stuff_documents_chain(llm = llm, prompt = prompt)\n",
    "    \n",
    "response = chain.invoke({ \"input\":\"Que es LCEL\",\"context\" :docs})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c1a142",
   "metadata": {},
   "source": [
    "### PARTE RETRIVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc2ac032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263\n",
      "LCEL, o LangChain Expression Language, es una plataforma declarativa diseÃ±ada para encadenar aplicaciones de modelos de lenguaje (LLM). Fue creada con el objetivo de facilitar la transiciÃ³n de prototipos a producciÃ³n sin necesidad de cambiar el cÃ³digo. LCEL permite construir desde las cadenas mÃ¡s simples, como \"prompt + LLM\", hasta las mÃ¡s complejas, con cientos de pasos, y ha demostrado ser efectiva en entornos de producciÃ³n.\n",
      "\n",
      "Algunas de las caracterÃ­sticas destacadas de LCEL incluyen:\n",
      "\n",
      "1. **Soporte de streaming de primera clase**: Todos los pasos en una cadena son automÃ¡ticamente registrados en LangSmith, lo que maximiza la capacidad de observaciÃ³n y depuraciÃ³n.\n",
      "2. **Despliegue sin problemas con LangServe**: Facilita el despliegue de las aplicaciones desarrolladas.\n",
      "\n",
      "En resumen, LCEL es una herramienta poderosa para desarrollar, probar, evaluar y monitorear aplicaciones de modelos de lenguaje, permitiendo una integraciÃ³n y despliegue eficientes en entornos de producciÃ³n.\n"
     ]
    }
   ],
   "source": [
    "#ya se cargÃ³ los vectores y se creo una basew da ddatos d eventores ahora toca poder encontrar los mas relevantes a mi pregunta\n",
    "# Lo que hace es carga todo los documentos, los separa en chuca, despeus cada chuk es vectorizado \n",
    "# y se gunarad en la base de datos, despues buca los chunks mas similares a la pregunta y esos los pasa como contexto\n",
    "# de sta forma se ahorra en el costo de tokens y no se sobrepasa los limites\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "def get_documents_from_web(url):\n",
    "    loader = WebBaseLoader(url)\n",
    "    docs = loader.load() \n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size = 200, chunk_overlap = 20)\n",
    "    splitDocs = splitter.split_documents(docs)\n",
    "    print(len(splitDocs))\n",
    "    return splitDocs\n",
    "\n",
    "\n",
    "\n",
    "def create_db(docs):\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorStore = FAISS.from_documents(docs, embedding = embeddings)\n",
    "    return vectorStore\n",
    "\n",
    "\n",
    "def create_chain(vectorStore):\n",
    "    llm = ChatOpenAI(model = \"gpt-4o\",\n",
    "                     temperature = 0.4, # mas cerca de 0 mas concreto mas cerca de 1 mas creativo\n",
    "                     verbose = True                \n",
    "                    )\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(\"\"\" Responde a las preguntas del ususario:\n",
    "                                                Contexto: {context} \n",
    "                                                Pregunta: {input}\n",
    "                                                \"\"\")\n",
    "    chain = create_stuff_documents_chain(llm = llm, prompt = prompt)\n",
    "    \n",
    "    retriever = vectorStore.as_retriever()\n",
    "    retrieval_chain = create_retrieval_chain(retriever,chain )\n",
    "    return retrieval_chain\n",
    "\n",
    "\n",
    "docs = get_documents_from_web(\"https://python.langchain.com/v0.2/docs/concepts/#langchain-expression-language\")\n",
    "vectorStore = create_db(docs)\n",
    "chain = create_chain(vectorStore)\n",
    "    \n",
    "response = chain.invoke({ \"input\":\"Que es LCEL\"})# \"context\" :docs --> sacamos\n",
    "\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd2528a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae493b88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81379692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bad9363",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
